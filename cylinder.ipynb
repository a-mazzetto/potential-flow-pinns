{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-mazzetto/potential-flow-pinns/blob/main/cylinder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Potential Flow past a Cylinder with Physics-Informed Neural Networks\n",
        "\n",
        "## Table of contents\n",
        "\n",
        "- [Introduction](#intro)\n",
        "- [Problem Setting and Analytical Solution](#problem)\n",
        "- [Physics-Informed Neural Network Solution](#pinns)\n",
        "- [Experiments](#experiments)\n",
        "- [Results](#results)\n",
        "- [Conclusions](#conclusions)\n",
        "- [References](#references)\n",
        "\n",
        "<a name=\"intro\"></a>\n",
        "## Introduction\n",
        "\n",
        "Physics-informed neural networks that solve Navier-Stokes equations are usually benchmarked against numerical solutions, given that in (almost) all applications the analytical solution is not available. There is a class of problems, however, that have numerical solutions [[1](#Ref1)] and could be used as a reference for deep learning solvers. Examples are the incompressible Couette, Poiseuille, Hagen-Poiseuille flows and the two-dimensional inviscid incompressible flow past a cylinder or a parametric Joukovskii aerofoil. In this post we base our analysis on the incompressible inviscid flow past a circular cylinder, and we will:\n",
        "- Introduce the problem setting, analytical solution and physics-informed neural network problem setup\n",
        "- Create and train a multi-layer perceptron with partial differential equations loss, impermeability loss and free stream loss\n",
        "\n",
        "This code was run on Google Colab with an A100 GPU for the deep learning model training and inference. In the following cells please find some initial imports and a code cell useful in Google Colab to mount a Google Drive folder."
      ],
      "metadata": {
        "id": "7HbhQTKJeLfd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASfzJzWNpY0K"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import Union\n",
        "import numpy as np\n",
        "from numpy.typing import NDArray\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TehWEhpP_ef"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "root = \"/content/drive/My Drive/cylinder/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_294htISJjFa"
      },
      "outputs": [],
      "source": [
        "def complex_to_array(ll: np.typing.ArrayLike):\n",
        "  return np.vstack((ll.real, ll.imag)).T"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"problem\"></a>\n",
        "## Problem Setting and Analytical Solution\n",
        "\n",
        "We shall consider the inviscid (fluid viscosity $\\nu=0$) incompressible (fluid density $\\rho$ is constant) flow past a circular cylinder in the two-dimensional space, which obey the special case of Navier-Stokes equations known as Euler equations <a name=\"euler_equations\"></a>:\n",
        "\n",
        "$$u \\frac{\\partial u}{\\partial x} + v \\frac{\\partial u}{\\partial y} + \\frac{1}{\\rho} \\frac{\\partial p}{\\partial x} = 0$$\n",
        "\n",
        "$$u \\frac{\\partial v}{\\partial x} + v \\frac{\\partial v}{\\partial y} + \\frac{1}{\\rho} \\frac{\\partial p}{\\partial y} = 0$$\n",
        "\n",
        "$$\\frac{\\partial u}{\\partial x} + \\frac{\\partial v}{\\partial y} = 0$$\n",
        "\n",
        "For simplicity and without loss of generality we will set $\\rho=1$. Let us set the cylinder radius $a=1$ and angle of attack $\\alpha$. The complex conjuugate of the velocity has an analytical expressione as follows [[1](#Ref1)]:\n",
        "\n",
        "$$\\bar{V}(z) = V_\\infty \\left( e^{-i \\alpha} - \\frac{a^2}{z^2 e^{-i \\alpha}}\\right)$$\n",
        "\n",
        "where $z = x + i y$ is a complex representation of the two-dimensional space and $V_\\infty$ is the far-field velocity. Using Bernoulli equation we can obtain the pressure field:\n",
        "\n",
        "$$p = \\rho \\frac{v_\\infty^2 - |V|^2}{2} + p_\\infty$$\n",
        "\n",
        "with $p_\\infty$ the far-field pressure.\n",
        "\n",
        "We define ageneral setting for solving two-dimensionalinviscid ncompressible flows of this sort, that are also called potential flows, and we define class for getting the boundary, filtering for the outer points and calculating the analytical velocity and pressure."
      ],
      "metadata": {
        "id": "pVy99oGlfU36"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ka3hG9SQfJtd"
      },
      "outputs": [],
      "source": [
        "class PotentialFlowShape(ABC):\n",
        "\n",
        "  @abstractmethod\n",
        "  def get_boundary(self, thetas: Union[float, NDArray[np.float64]], **kwargs):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  @abstractmethod\n",
        "  def filter_outer(self, z: NDArray, **kwargs):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  @abstractmethod\n",
        "  def velocity_field(self, z: NDArray, **kwargs):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  @abstractmethod\n",
        "  def pressure_field(self, v: NDArray, **kwargs):\n",
        "    raise NotImplementedError()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a cylinder section, in particular, we define the boundary as the circumference. We also calculate the normal to the boundary, that we will se it is useful to define the impermeability condition. Velocity and pressure are defined as explained before."
      ],
      "metadata": {
        "id": "lQJEdgAxL5yN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Cylinder(PotentialFlowShape):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            radius: float=1.0,\n",
        "            v_inf: float=1.0,\n",
        "            p_inf: float=1.0,\n",
        "            rho: float=1.0,\n",
        "            alpha: float=0.0,\n",
        "            ):\n",
        "        self.radius = radius\n",
        "        self.v_inf = v_inf\n",
        "        self.p_inf = p_inf\n",
        "        self.rho = rho\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def get_boundary(self, thetas: Union[float, NDArray[np.float64]]):\n",
        "      \"\"\"Sample from circle: vector and normal\"\"\"\n",
        "      vector = self.radius * np.cos(thetas) + 1j * self.radius * np.sin(thetas)\n",
        "      return vector, vector / np.absolute(vector)\n",
        "\n",
        "    def filter_outer(self, z, strict: bool=False):\n",
        "      bouter = np.absolute(z) > self.radius if strict else np.absolute(z) >= self.radius\n",
        "      return bouter\n",
        "\n",
        "    def velocity_field(self, z):\n",
        "      \"\"\"Cylinder velocity field\"\"\"\n",
        "      rot = np.exp(-1j * self.alpha)\n",
        "      v_vec_cc = self.v_inf * (rot - 1 / rot * self.radius**2 / z**2) # Complex conjugate of velocity\n",
        "      v_vec = np.conjugate(v_vec_cc)\n",
        "      return v_vec\n",
        "\n",
        "    def pressure_field(self, v):\n",
        "      \"\"\"Pressure scalar field\"\"\"\n",
        "      v_mod = np.absolute(v)\n",
        "      return self.rho * (self.v_inf**2 - v_mod**2) / 2 + self.p_inf\n"
      ],
      "metadata": {
        "id": "JLN5Lq2PLmag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define tho helper functions to plot the flow in a grid and to inspect the boundary."
      ],
      "metadata": {
        "id": "zlv8zRynMX4I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwcDxIUXDfHc"
      },
      "outputs": [],
      "source": [
        "def original_mesh_data(shape: PotentialFlowShape, bound: float, npoints: int):\n",
        "  grid = np.linspace(-bound, bound, npoints)\n",
        "  mesh = np.meshgrid(grid, grid)\n",
        "  x_mesh = mesh[0].flatten()\n",
        "  y_mesh = mesh[1].flatten()\n",
        "  z_mesh = x_mesh + 1j * y_mesh\n",
        "  b_mesh_outer = shape.filter_outer(z_mesh)\n",
        "  z_mesh = z_mesh[b_mesh_outer]\n",
        "\n",
        "  v_mesh = shape.velocity_field(z_mesh)\n",
        "  p_mesh = shape.pressure_field(v_mesh)\n",
        "\n",
        "  v_mesh_mod = np.absolute(v_mesh)\n",
        "  v_mesh_angle = np.angle(v_mesh, deg=True)\n",
        "\n",
        "  return z_mesh, v_mesh, p_mesh\n",
        "\n",
        "def original_boundary_data(shape: PotentialFlowShape, npoints: int):\n",
        "  theta_boundary = np.linspace(0, 2 * np.pi, npoints)\n",
        "  z_boundary, n_boundary = shape.get_boundary(theta_boundary)\n",
        "  v_boundary = shape.velocity_field(z_boundary)\n",
        "  p_boundary = shape.pressure_field(v_boundary)\n",
        "\n",
        "  return z_boundary, n_boundary, v_boundary, p_boundary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is time to visualize the flow after initializing a cylinder with angle of attack 20 degrees."
      ],
      "metadata": {
        "id": "fmwBpD-_Mi4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cylinder = Cylinder(alpha=20.0 * np.pi / 180.0)\n",
        "\n",
        "z_mesh, v_mesh, p_mesh = original_mesh_data(cylinder, 2, 50)\n",
        "z_boundary, n_boundary, v_boundary, p_boundary = original_boundary_data(cylinder, 40)"
      ],
      "metadata": {
        "id": "D_A97ZNYMgPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us start by visualizing the boundary and its normals, then the $v_x$ and $v_y$ velocity components, and finally the velocity field."
      ],
      "metadata": {
        "id": "9HVsXputNqpR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dQBuSoLfAhD"
      },
      "outputs": [],
      "source": [
        "plt.scatter(z_mesh.real, z_mesh.imag, marker=\".\", linewidths=0.05)\n",
        "plt.plot(z_boundary.real, z_boundary.imag, color=\"red\")\n",
        "plt.quiver(z_boundary.real, z_boundary.imag, n_boundary.real, n_boundary.imag, color=\"red\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.gca().set_aspect(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lAdEPCSfEX9"
      },
      "outputs": [],
      "source": [
        "v_mesh_mod = np.absolute(v_mesh)\n",
        "\n",
        "plt.plot(z_boundary.real, z_boundary.imag, color=\"red\")\n",
        "plt.scatter(z_mesh.real, z_mesh.imag, c=v_mesh_mod, marker=\".\", linewidths=0.05)\n",
        "plt.colorbar()\n",
        "plt.title(\"$v_x$\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.gca().set_aspect(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQXWtsZCfQU1"
      },
      "outputs": [],
      "source": [
        "plt.plot(z_boundary.real, z_boundary.imag, color=\"red\")\n",
        "plt.scatter(z_mesh.real, z_mesh.imag, c=p_mesh, marker=\".\", linewidths=0.05)\n",
        "plt.colorbar()\n",
        "plt.title(\"$v_y$\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.gca().set_aspect(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqLGW7bCfS6o"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(z_boundary.real, z_boundary.imag, color=\"red\")\n",
        "plt.quiver(z_mesh.real, z_mesh.imag, v_mesh.real, v_mesh.imag, v_mesh_mod)\n",
        "plt.colorbar()\n",
        "plt.title(\"$v$\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.gca().set_aspect(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should note inp particular what happens at the boundary. For an inviscid flow we do ot define a sticky boundary, but an impermeable boundary instead: the velocity at the boundary must be tangent to the boundary itself. This is satisfied in the analytical solution as visualized below."
      ],
      "metadata": {
        "id": "VWVdW7xfPRQW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q45sGsQfXcu"
      },
      "outputs": [],
      "source": [
        "plt.plot(z_boundary.real, z_boundary.imag, color=\"red\")\n",
        "plt.quiver(z_boundary.real, z_boundary.imag, v_boundary.real, v_boundary.imag)\n",
        "plt.colorbar()\n",
        "plt.title(\"$v$\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.gca().set_aspect(1)\n",
        "\n",
        "# Track mean dot product\n",
        "np.mean(np.sum(complex_to_array(n_boundary) * complex_to_array(v_boundary), axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A final remark on the flow past a cylinder is the positioning of the stagnation points at -160° and 20°. We wish to verify the position of the stagnation points in the deep learning solution as well. The position will be found using `scipy`'s `fsolve`. As a double check we find numerically the stagnation points in the analytical solution, and we find their position as expected."
      ],
      "metadata": {
        "id": "WEM4Fv1IPvGC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7essZCUQoSqe"
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import fsolve\n",
        "\n",
        "def stagnation_point_search(shape: PotentialFlowShape):\n",
        "\n",
        "  z_boundary, _, v_boundary, _ = original_boundary_data(shape, 50)\n",
        "\n",
        "  first_argmin = np.argmin(np.absolute(v_boundary))\n",
        "  first_theta_min = np.angle(z_boundary[first_argmin])\n",
        "\n",
        "  second_argmin = np.argmin(np.absolute(np.delete(v_boundary, first_argmin)))\n",
        "  second_theta_min = np.angle(z_boundary[second_argmin])\n",
        "\n",
        "  def internal_fn(t):\n",
        "    z, _ = shape.get_boundary(t)\n",
        "    v_mod = np.absolute(shape.velocity_field(z))\n",
        "\n",
        "    return v_mod\n",
        "\n",
        "  first_stagnation = fsolve(internal_fn, first_theta_min)\n",
        "  second_stagnation = fsolve(internal_fn, second_theta_min)\n",
        "  return np.array((first_stagnation, second_stagnation))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcFWkVvsm5aO"
      },
      "outputs": [],
      "source": [
        "stagnation_point_search(cylinder) * 180/np.pi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"pinns\"></a>\n",
        "## Physics-Informed Neural Network Solution\n",
        "\n",
        "We wish to traing a neural network $(v_x, v_y, p) = u(x, y)$. Our choice for $u$ is a multi-layer perceptron with 2 inputs, 3 outputs and 8 hiddel layers of dimension 128. We create a helper function `get_mlp` to create a multi-layer perceptron with PyTorch. Given that to calculate the partial differential equations losses we need the derivatives of the outputs with respect to $x$ and $y$ we create a helper function `calculate_derivatives` to calculate such derivatives with automatic differentiation."
      ],
      "metadata": {
        "id": "VjlrZIjpYHgg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjAUXjqxfiBr"
      },
      "outputs": [],
      "source": [
        "from typing import Union, List\n",
        "from itertools import chain\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def get_mlp(\n",
        "    dimensions: List[int],\n",
        "    hidden_activations: nn.Module = nn.SiLU,\n",
        "    output_activation: nn.Module = nn.Identity,\n",
        "    dtype = torch.float64,\n",
        "    ):\n",
        "  \"\"\"MultiLayerPerceptron\"\"\"\n",
        "  n = len(dimensions)\n",
        "  layers = [nn.Linear(dimensions[i], dimensions[i + 1], dtype=dtype) for i in range(n - 1)]\n",
        "  activations = (n - 2) * [hidden_activations()] + [output_activation()]\n",
        "  return nn.Sequential(*chain(*zip(layers, activations)))\n",
        "\n",
        "def calculate_derivatives(model, x, second=False):\n",
        "    \"\"\"Note: the second order derivative fails if independent upon X. Try for example\n",
        "    the second order derivative of nn.Linear\"\"\"\n",
        "\n",
        "    x.requires_grad = True\n",
        "    assert x.is_leaf\n",
        "\n",
        "    y = model(x)\n",
        "\n",
        "    dys = []\n",
        "    ddys = []\n",
        "\n",
        "    for _y in torch.split(y, 1, dim=1):\n",
        "      dy = torch.autograd.grad(_y, x, torch.ones_like(_y), create_graph=True)[0]\n",
        "      dy.retain_grad()\n",
        "      dys.append(dy)\n",
        "      if second:\n",
        "        for _dy in torch.split(dy, 1, dim=1):\n",
        "            ddy = torch.autograd.grad(_dy, x, torch.ones_like(_dy), retain_graph=True)[0]\n",
        "            ddys.append(ddy)\n",
        "    if second:\n",
        "        return y, torch.cat(dys, dim=1), torch.cat(ddys, dim=1)\n",
        "    return y, torch.cat(dys, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W4NjEY2fdw7"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Inputs: (x, y)\n",
        "# Outputs: (u, v, p)\n",
        "mlp = get_mlp([2] + 8 * [128] + [3])\n",
        "mlp.to(device)\n",
        "mlp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we created our model. Now we need to create the losses that will give awareness of the geometry and constraints of our problem. First of all we need to satisfy the [Euler Equations](#euler_equations). We create a `PDELoss` that incorporates such equations, and test it."
      ],
      "metadata": {
        "id": "uTOFfcIjCsb8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcJ9V4jlffPs"
      },
      "outputs": [],
      "source": [
        "class PDELoss(torch.nn.Module):\n",
        "    \"\"\"Problem Loss\"\"\"\n",
        "    def __init__(self, rho=1.0):\n",
        "        super().__init__()\n",
        "        self.rho = rho\n",
        "\n",
        "    def forward(self, y, dydx, w_c: torch.Tensor=None, w_mx: torch.Tensor=None, w_my: torch.Tensor=None, dtype: torch.TensorType=None):\n",
        "        \"\"\"y = (u, v, p, du/dx, du/dy, dv/dx, dv/dy, dp/dx, dp/dy)\"\"\"\n",
        "        dtype = dtype if dtype is not None else torch.float64\n",
        "        w_c = w_c if w_c is not None else torch.tensor(1.0, dtype=dtype)\n",
        "        w_mx = w_mx if w_mx is not None else torch.tensor(1.0, dtype=dtype)\n",
        "        w_my = w_my if w_my is not None else torch.tensor(1.0, dtype=dtype)\n",
        "\n",
        "        u, v, p = torch.split(y, 1, dim=1)\n",
        "        dudx, dudy, dvdx, dvdy, dpdx, dpdy = torch.split(dydx, 1, dim=1)\n",
        "\n",
        "        pde_mx = u * dudx + v * dudy + dpdx / self.rho\n",
        "        pde_my = u * dvdx + v * dvdy + dpdy / self.rho\n",
        "        pde_c = dudx + dvdy\n",
        "\n",
        "        return (\n",
        "            torch.nn.functional.mse_loss(pde_mx, torch.zeros_like(w_mx * pde_mx)),\n",
        "            torch.nn.functional.mse_loss(pde_my, torch.zeros_like(w_my * pde_my)),\n",
        "            torch.nn.functional.mse_loss(pde_c, torch.zeros_like(w_c * pde_c)),\n",
        "        )\n",
        "\n",
        "pde_loss = PDELoss().to(device)\n",
        "\n",
        "x_test = torch.tensor([[1.0, 0.0]], dtype=torch.float64).to(device)\n",
        "pde_loss(*calculate_derivatives(mlp, x_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secondly, we need to constrain the model based on the geometry. The condition on the cylinder boundary is that the flow should not permeate: in other words, the velocity vector and normal to the boundary should be perpendicular. The `ImpermeabilityLoss` defined below is in fact a dot product whoich should be zero on the boundary."
      ],
      "metadata": {
        "id": "6wOLxLuzDb3_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wABLXFrfmNW"
      },
      "outputs": [],
      "source": [
        "class ImpermeabilityLoss(torch.nn.Module):\n",
        "    \"\"\"Impermeability loss\"\"\"\n",
        "\n",
        "    def forward(self, normals, outputs, w: torch.Tensor=None, dtype: torch.TensorType=None):\n",
        "      dtype = dtype if dtype is not None else torch.float64\n",
        "\n",
        "      w = w if w is not None else torch.tensor(1.0, dtype=dtype)\n",
        "\n",
        "      x, y = torch.split(normals, 1, dim=1)\n",
        "      u, v, p = torch.split(outputs, 1, dim=1)\n",
        "\n",
        "      dot_product = w * (x * u + y * v)\n",
        "\n",
        "      return torch.nn.functional.mse_loss(dot_product, torch.zeros_like(dot_product))\n",
        "\n",
        "impermeability_loss = ImpermeabilityLoss().to(device)\n",
        "\n",
        "test_tensor = torch.tensor([[1.0, 0.0]], dtype=torch.float64).to(device)\n",
        "test_result = mlp(test_tensor)\n",
        "impermeability_loss(test_tensor, test_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we defined what is the condition for the flow at any point (Euler equations) and what is the condition on the boundary. To complete the description of our problem we need to define which is the velocity afar. This is called free stream condition, and the velocity in a region far prom the object is simply dictated."
      ],
      "metadata": {
        "id": "yeU064a9D-Au"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIcCAu5EgDdX"
      },
      "outputs": [],
      "source": [
        "class FreeStreamLoss(torch.nn.Module):\n",
        "    \"\"\"Impermeability loss\"\"\"\n",
        "    def __init__(self, v_inf, alpha, p_inf):\n",
        "        super().__init__()\n",
        "        self.vx_inf = torch.tensor(v_inf * np.cos(alpha))\n",
        "        self.vy_inf = torch.tensor(v_inf * np.sin(alpha))\n",
        "        self.p_inf = p_inf\n",
        "\n",
        "    def forward(self, outputs):\n",
        "        vx, vy, p = torch.split(outputs, 1, dim=1)\n",
        "        return (\n",
        "            torch.nn.functional.mse_loss(vx, self.vx_inf * torch.ones_like(vx)) +\n",
        "            torch.nn.functional.mse_loss(vy, self.vy_inf * torch.ones_like(vy)),\n",
        "            torch.nn.functional.mse_loss(p, self.p_inf * torch.ones_like(p))\n",
        "        )\n",
        "\n",
        "freestream_loss = FreeStreamLoss(v_inf=cylinder.v_inf, alpha=cylinder.alpha, p_inf=cylinder.p_inf).to(device)\n",
        "\n",
        "test_tensor = torch.tensor([[1.0, 0.0]], dtype=torch.float64).to(device)\n",
        "test_result = mlp(test_tensor)\n",
        "freestream_loss(test_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For completeness, the potential flow under consideration also has a constraint called _Kutta condition_ that imposes no circulation of the flow at the trailing edge. Our problem is symmetric, and this condition is automatically satisfied. Hence, we do not need to impose the Kutta condition like we would need to do for other problems like the Joukovskii aerofoil.\n",
        "\n",
        "After defining the loss functions we are left with an important task that is to generate the datapoints on which to calculate the various loss terms. We need to:\n",
        "- Generate points afar from the object: we call that horizon\n",
        "- Generate points on the object boundary\n",
        "- Generate points in between\n",
        "We now describe such functions.\n",
        "\n",
        "We would like to point out two aspects here:\n",
        "1. We wish to generate new data points at each training iteration, like if our dataset was virtually infinite\n",
        "2. We generate the random data with Sobol sequences to cover the space more uniformly [[2](#sobol)] by default\n",
        "\n",
        "The function `generate_boundary` samples points on the boundary and returns their coordinates as well as the cooresponding normal vectors.\n",
        "\n",
        "The function `generate_exterior` samples randomly in a square and returns only the points outside the shape.\n",
        "\n",
        "The function `generate_horizon` samples points on a circle.\n",
        "\n",
        "Finally, the function `generate_data` generates the data for our probloem specifically, using the above functions. As we wish to test different horizons (i.e. different free stream conditions ad various distances from the shape), we define a maximum horizon to sample always the same points with `generate_exterior` and then restrich ourselves to the actual horizon in consideration."
      ],
      "metadata": {
        "id": "ejFxyN1BEf7j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjjn8UodkHKS"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import qmc\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "sobol_sampler_1d = qmc.Sobol(d=1, rng=np.random.default_rng(seed=0), bits=64)\n",
        "sobol_sampler_2d = qmc.Sobol(d=2, rng=np.random.default_rng(seed=0), bits=64)\n",
        "\n",
        "def generate_boundary(\n",
        "    shape: PotentialFlowShape,\n",
        "    n_boundary_samples: int=1000,\n",
        "    use_sobol: bool=True,\n",
        "    seed: int=0,\n",
        "):\n",
        "  if use_sobol:\n",
        "    sampler = qmc.Sobol(d=1, rng=np.random.default_rng(seed=seed), bits=64)\n",
        "    boundary_theta_sample = qmc.scale(\n",
        "        sampler.random(n=n_boundary_samples), 0, 2 * np.pi).flatten()\n",
        "  else:\n",
        "    rng = np.random.default_rng(seed=seed)\n",
        "    boundary_theta_sample = rng.uniform(0, 2 * np.pi, n_boundary_samples)\n",
        "  boundary_points, boundary_normals = shape.get_boundary(boundary_theta_sample)\n",
        "  boundary_points = complex_to_array(boundary_points)\n",
        "  boundary_normals = complex_to_array(boundary_normals)\n",
        "  return boundary_points, boundary_normals\n",
        "\n",
        "def generate_exterior(\n",
        "    shape: PotentialFlowShape,\n",
        "    r_horizon: float=7,\n",
        "    rho_exterior_samples: int=1000,\n",
        "    use_sobol: bool=True,\n",
        "    seed: int=0,\n",
        "):\n",
        "  n_exterior_samples = int(rho_exterior_samples * r_horizon**2)\n",
        "  if use_sobol:\n",
        "    sampler = qmc.Sobol(d=2, rng=np.random.default_rng(seed=seed), bits=64)\n",
        "    exterior_sample = qmc.scale(sampler.random(n=n_exterior_samples), -r_horizon, r_horizon)\n",
        "  else:\n",
        "    rng = np.random.default_rng(seed=seed)\n",
        "    exterior_sample = rng.uniform(-r_horizon, r_horizon, size=(n_exterior_samples, 2))\n",
        "  exterior_sample_absolute = np.sqrt(np.sum(exterior_sample**2, axis=1))\n",
        "  exterior_sample_complex = exterior_sample[:, 0] + 1j * exterior_sample[:, 1]\n",
        "  valid_exterior_sample = cylinder.filter_outer(exterior_sample_complex) & (exterior_sample_absolute < r_horizon)\n",
        "  exterior_points = exterior_sample[valid_exterior_sample]\n",
        "  return exterior_points\n",
        "\n",
        "def generate_horizon(\n",
        "    shape: PotentialFlowShape,\n",
        "    r_horizon: float=7,\n",
        "    rho_horizon_samples: int=1000,\n",
        "    use_sobol: bool=False,\n",
        "    seed: int=0,\n",
        "):\n",
        "  n_horizon_samples = int(rho_horizon_samples * r_horizon / 2)\n",
        "  if use_sobol:\n",
        "    sampler = qmc.Sobol(d=1, rng=np.random.default_rng(seed=seed), bits=64)\n",
        "    horizon_theta_sample = qmc.scale(sampler.random(n=n_horizon_samples), 0, 2 * np.pi).flatten()\n",
        "  else:\n",
        "    rng =np.random.default_rng(seed=seed)\n",
        "    horizon_theta_sample = rng.uniform(0, 2 * np.pi, 1000)\n",
        "  horizon_points = np.vstack(\n",
        "      (r_horizon * np.cos(horizon_theta_sample),\n",
        "       r_horizon * np.sin(horizon_theta_sample)\n",
        "       )\n",
        "      ).T\n",
        "  return horizon_points\n",
        "\n",
        "def generate_data(\n",
        "    r_horizon: float=7,\n",
        "    r_horizon_max: float=10,\n",
        "    n_boundary_samples: int=1000,\n",
        "    rho_exterior_samples: int=1000,\n",
        "    rho_horizon_samples: int=1000,\n",
        "    r_pde_downweight: float=3,\n",
        "    use_sobol: bool=False,\n",
        "    ):\n",
        "  boundary_points, boundary_normals = generate_boundary(\n",
        "      cylinder,\n",
        "      n_boundary_samples=n_boundary_samples,\n",
        "      use_sobol=use_sobol,\n",
        "  )\n",
        "\n",
        "  exterior_points = generate_exterior(\n",
        "      cylinder,\n",
        "      r_horizon=r_horizon_max,\n",
        "      rho_exterior_samples=rho_exterior_samples,\n",
        "      use_sobol=use_sobol,\n",
        "  )\n",
        "  exterior_points= exterior_points[\n",
        "      np.sqrt(np.sum(exterior_points**2, axis=1)) < r_horizon\n",
        "  ]\n",
        "\n",
        "  horizon_points = generate_horizon(\n",
        "      cylinder,\n",
        "      r_horizon=r_horizon,\n",
        "      rho_horizon_samples=rho_horizon_samples,\n",
        "      use_sobol=use_sobol,\n",
        "  )\n",
        "\n",
        "  pde_points = np.concatenate((boundary_points, exterior_points, horizon_points))\n",
        "  delta_points = r_horizon**2 / r_pde_downweight**2\n",
        "  pde_weights = np.where(np.sqrt((pde_points**2).sum()) < r_pde_downweight, 1 - 1 / delta_points, 1 / delta_points)\n",
        "\n",
        "  return pde_points, pde_weights, boundary_points, boundary_normals, horizon_points\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We test our data generating function and plot the sample."
      ],
      "metadata": {
        "id": "vxvhgmJTOqgs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nyi69lYmkJ9S"
      },
      "outputs": [],
      "source": [
        "pde_points, pde_weights, boundary_points, boundary_normals, horizon_points = generate_data(r_horizon=3, use_sobol=True)\n",
        "print(pde_points.shape)\n",
        "\n",
        "plt.scatter(pde_points[:, 0], pde_points[:, 1], linewidths=0.01)\n",
        "plt.scatter(boundary_points[:, 0], boundary_points[:, 1], linewidths=0.01)\n",
        "plt.quiver(boundary_points[:, 0], boundary_points[:, 1], boundary_normals[:, 0], boundary_normals[:, 1])\n",
        "plt.scatter(horizon_points[:, 0], horizon_points[:, 1], linewidths=0.01)\n",
        "plt.gca().set_aspect(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Physics-informed neural networks have multiple loss terms for differential equations and boundary conditions. Instead of defining the relative loss weights a priori, there are schemas whereby the loss term weights are updated iteratively. One of these is the SoftAdapt [[3](#softadapt)] loss weight update, implemented below. We wish to test it in our problem."
      ],
      "metadata": {
        "id": "GOAI87PVOvmf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id-pXyJrRCrN"
      },
      "outputs": [],
      "source": [
        "class SoftAdaptLossAggregation(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.register_buffer(\"previous_losses\", None, persistent=False)\n",
        "\n",
        "  def forward(self, losses: torch.types.Tensor):\n",
        "    if self.previous_losses is None:\n",
        "      weights = torch.ones_like(losses) / torch.numel(losses)\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        p = losses / (self.previous_losses + self.eps)\n",
        "        weights = torch.softmax(p - p.max(), dim=0)\n",
        "    loss = torch.dot(losses, weights)\n",
        "    self.previous_losses = losses\n",
        "    return loss, losses, weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we train our model with an Adam optimizer. The training loop is standard: training data is sampled at each iteration, the losses calculated and the model weights optimised and updated."
      ],
      "metadata": {
        "id": "jyGNo6YMQ2xs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYSDPAKtgDuu"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "# %% Training\n",
        "def train_one_batch(model, r_horizon: float, loss_agg_fn = None):\n",
        "    # Zero your gradients for every batch!\n",
        "\n",
        "    # Make predictions for this batch\n",
        "    pde_points, pde_weights, boundary_points, boundary_normals, horizon_points = generate_data(\n",
        "        use_sobol=True,\n",
        "        r_horizon=r_horizon,\n",
        "    )\n",
        "    pde_points = torch.tensor(pde_points, dtype=torch.float64).to(device)\n",
        "    pde_weights = torch.tensor(pde_weights, dtype=torch.float64).to(device)\n",
        "    boundary_points = torch.tensor(boundary_points, dtype=torch.float64).to(device)\n",
        "    boundary_normals = torch.tensor(boundary_normals, dtype=torch.float64).to(device)\n",
        "    horizon_points = torch.tensor(horizon_points, dtype=torch.float64).to(device)\n",
        "\n",
        "    pde_pred = calculate_derivatives(model, pde_points)\n",
        "    boundary_pred = model(boundary_points)\n",
        "    horizon_pred = model(horizon_points)\n",
        "\n",
        "    # Compute the loss and its gradients\n",
        "    loss_mx, loss_my, loss_c = pde_loss(*pde_pred, w_c=pde_weights, w_mx=pde_weights, w_my=pde_weights)\n",
        "    loss_imp = impermeability_loss(boundary_normals, boundary_pred)\n",
        "    loss_free_v, loss_free_p = freestream_loss(horizon_pred)\n",
        "\n",
        "    current_losses = torch.stack((loss_mx, loss_my, loss_c, loss_imp, loss_free_v, loss_free_p))\n",
        "    if loss_agg_fn is not None:\n",
        "      loss, _, current_loss_weights = loss_agg_fn(current_losses)\n",
        "    else:\n",
        "      current_loss_weights = torch.ones_like(current_losses)\n",
        "      loss = torch.dot(current_loss_weights, current_losses)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # Adjust learning weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print loss\n",
        "    return (\n",
        "        loss.detach().cpu().numpy(),\n",
        "        current_losses.detach().cpu().numpy(),\n",
        "        current_loss_weights.detach().cpu().numpy(),\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub-DtZw6gIYD"
      },
      "outputs": [],
      "source": [
        "train_one_batch(mlp, r_horizon=3, loss_agg_fn=SoftAdaptLossAggregation())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Throughout training we wish to validate our model on the same grid of data. We create a helper function for model predictions and a helper function to calculate validation mean absolute errors."
      ],
      "metadata": {
        "id": "sGz4u6YPRbOk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFCthfbes7O3"
      },
      "outputs": [],
      "source": [
        "def predict(model: torch.nn.Module, z: np.typing.NDArray[np.complex64]):\n",
        "  pred = model(\n",
        "      torch.tensor(complex_to_array(z), dtype=torch.float64).to(device)\n",
        "  )\n",
        "  v = pred[:, 0].cpu().detach().numpy() + 1j * pred[:, 1].cpu().detach().numpy()\n",
        "  p = pred[:, 2].cpu().detach().numpy()\n",
        "\n",
        "  return v, p\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_khDKZ9LgUL"
      },
      "outputs": [],
      "source": [
        "def validation_metrics(\n",
        "    v_mesh: NDArray[np.complex64],\n",
        "    v_mesh_pred: NDArray[np.complex64],\n",
        "    p_mesh:NDArray[np.complex64],\n",
        "    p_mesh_pred:NDArray[np.complex64],\n",
        "    n_boundary: NDArray[np.complex64],\n",
        "    v_boundary_pred: NDArray[np.complex64],\n",
        "):\n",
        "  vx_mae = np.abs(v_mesh.real - v_mesh_pred.real).mean()\n",
        "  vy_mae = np.abs(v_mesh.imag - v_mesh_pred.imag).mean()\n",
        "  p_mae = np.abs(p_mesh - p_mesh_pred).mean()\n",
        "  impermeability_error = np.mean(\n",
        "      np.sum(\n",
        "          complex_to_array(n_boundary) * complex_to_array(v_boundary_pred),\n",
        "          axis=1,\n",
        "          )\n",
        "      )\n",
        "\n",
        "  return vx_mae, vy_mae, p_mae, impermeability_error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the code to launch model training. Some parameters are configurable in this cell:\n",
        "\n",
        "- `bsoftadaptloss`: whether to use the SoftAdapt loss weights\n",
        "- `r_horizon`: horizon with respect to the characteristic length $1$. The theoretical horizon is $\\infty$ but in practice we have to try finite horizons\n",
        "- `budget`\n",
        "- `validation_interval`: validation frequency"
      ],
      "metadata": {
        "id": "IIORdG74R-PC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBQFSyZxgNZw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from copy import deepcopy\n",
        "import tqdm as tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "bsoftadaptloss = False\n",
        "softadaptloss = SoftAdaptLossAggregation()\n",
        "r_horizon = 5\n",
        "\n",
        "name_id = \"final_horizon_5_45kit\"\n",
        "log_dir = os.path.join(\"/content/\", name_id)\n",
        "writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "budget = 45000\n",
        "validation_interval = 1000\n",
        "best_loss = 1e9\n",
        "\n",
        "model_for_validation = deepcopy(mlp)\n",
        "\n",
        "history = []\n",
        "pbar = tqdm.tqdm(range(budget))\n",
        "for epoch in pbar:\n",
        "    _h = train_one_batch(mlp, r_horizon=r_horizon, loss_agg_fn=(softadaptloss if bsoftadaptloss else None))\n",
        "\n",
        "    # print(_h[1], _h[2])\n",
        "\n",
        "    writer.add_scalar(\"Loss/loss\", _h[0], epoch)\n",
        "\n",
        "    writer.add_scalar(\"Loss/pde_mx_loss\", _h[1][0], epoch)\n",
        "    writer.add_scalar(\"Loss/pde_my_loss\", _h[1][1], epoch)\n",
        "    writer.add_scalar(\"Loss/pde_c_loss\", _h[1][2], epoch)\n",
        "    writer.add_scalar(\"Loss/impermeability_loss\", _h[1][3], epoch)\n",
        "    writer.add_scalar(\"Loss/freeflow_v_loss\", _h[1][4], epoch)\n",
        "    writer.add_scalar(\"Loss/freeflow_p_loss\", _h[1][5], epoch)\n",
        "\n",
        "    writer.add_scalar(\"Loss/pde_mx_loss_w\", _h[2][0], epoch)\n",
        "    writer.add_scalar(\"Loss/pde_my_loss_w\", _h[2][1], epoch)\n",
        "    writer.add_scalar(\"Loss/pde_c_loss_w\", _h[2][2], epoch)\n",
        "    writer.add_scalar(\"Loss/impermeability_loss_w\", _h[2][3], epoch)\n",
        "    writer.add_scalar(\"Loss/freeflow_v_loss_w\", _h[2][4], epoch)\n",
        "    writer.add_scalar(\"Loss/freeflow_p_loss_w\", _h[2][5], epoch)\n",
        "\n",
        "    if _h[0] < best_loss:\n",
        "      best_loss = _h[0]\n",
        "      best_model_state_dict = deepcopy(mlp.state_dict())\n",
        "\n",
        "    # Validation\n",
        "    if epoch % validation_interval == 0:\n",
        "      model_for_validation.load_state_dict(best_model_state_dict)\n",
        "\n",
        "      _, v_mesh, p_mesh = original_mesh_data(cylinder, 2, 50)\n",
        "      _, n_boundary, _, _ = original_boundary_data(cylinder, 40)\n",
        "\n",
        "      v_mesh_pred, p_mesh_pred = predict(model_for_validation, z_mesh)\n",
        "      v_boundary_pred, _ = predict(model_for_validation, z_boundary)\n",
        "\n",
        "      vx_mae, vy_mae, p_mae, impermeability_error = validation_metrics(\n",
        "          v_mesh,\n",
        "          v_mesh_pred,\n",
        "          p_mesh,\n",
        "          p_mesh_pred,\n",
        "          n_boundary,\n",
        "          v_boundary_pred,\n",
        "      )\n",
        "\n",
        "      writer.add_scalar(\"Validation/vx_mae\", vx_mae, epoch)\n",
        "      writer.add_scalar(\"Validation/vy_mae\", vy_mae, epoch)\n",
        "      writer.add_scalar(\"Validation/p_mae\", p_mae, epoch)\n",
        "      writer.add_scalar(\"Validation/impermeability_error\", impermeability_error, epoch)\n",
        "\n",
        "      pbar.set_postfix({\"vx_mae\": vx_mae, \"vy_mae\": vy_mae, \"p_mae\": p_mae, \"imp_error\": impermeability_error})\n",
        "\n",
        "    history.append(_h)\n",
        "\n",
        "mlp.load_state_dict(best_model_state_dict)\n",
        "torch.save(mlp, log_dir + \"/best_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b46focKR2tLV"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "shutil.copytree(log_dir, root + name_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tJQfizSlbZW"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# name_id = \"final_horizon_5_45kit\"\n",
        "\n",
        "# log_dir = os.path.join(\"/content/drive/My Drive/cylinder/\", name_id)\n",
        "\n",
        "# model_path = log_dir + \"/best_model.pt\"\n",
        "# model_path\n",
        "\n",
        "# mlp = torch.load(model_path, weights_only=False) # map_location=torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ02Eodw0ogR"
      },
      "source": [
        "# Diagnostics\n",
        "\n",
        "Below are some functions to compare predictions and analytical solution. We wish to compare the velcoity fields, the pressure field, the impermeability condition and the stagnation points positioning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU8ojYSpDZqi"
      },
      "outputs": [],
      "source": [
        "from matplotlib import colors\n",
        "\n",
        "def plot_flow_field_comparison(\n",
        "    z_boundary: NDArray[np.complex64],\n",
        "    true_vectors: NDArray[np.complex64],\n",
        "    predicted_vectors: NDArray[np.complex64]\n",
        "    ):\n",
        "  plt.ioff()\n",
        "\n",
        "  true_absolute = np.absolute(v_mesh)\n",
        "  predicted_absolute = np.absolute(predicted_vectors)\n",
        "\n",
        "  fig, axs = plt.subplots(1, 3, figsize=(15, 11))\n",
        "\n",
        "  norm = colors.Normalize(vmin=true_absolute.min(), vmax=true_absolute.max())\n",
        "\n",
        "  axs[0].set_title(\"True Flow Field\")\n",
        "  axs[0].plot(z_boundary.real, z_boundary.imag, color=\"red\")\n",
        "  quiver_0 = axs[0].quiver(z_mesh.real, z_mesh.imag, v_mesh.real, v_mesh.imag, np.absolute(v_mesh), norm=norm)\n",
        "  axs[0].set_aspect(1)\n",
        "\n",
        "  axs[1].set_title(\"Predicted Flow Field\")\n",
        "  axs[1].plot(z_boundary.real, z_boundary.imag, color=\"red\")\n",
        "  quiver_1 = axs[1].quiver(z_mesh.real, z_mesh.imag, v_mesh_pred.real, v_mesh_pred.imag, np.absolute(v_mesh_pred), norm=norm)\n",
        "  axs[1].set_aspect(1)\n",
        "\n",
        "  axs[2].set_axis_off()\n",
        "  fig.colorbar(quiver_0, ax=axs[2], orientation='vertical', fraction=1, shrink=0.5)\n",
        "\n",
        "  return fig\n",
        "\n",
        "def plot_mesh_comparison(\n",
        "    z_boundary: NDArray[np.complex64],\n",
        "    z_mesh: NDArray[np.complex64],\n",
        "    true_quantity: NDArray[np.float64],\n",
        "    predicted_quantity: NDArray[np.float64],\n",
        "    quantity_name: str):\n",
        "  plt.ioff()\n",
        "\n",
        "  fig, axs = plt.subplots(2, 3, figsize=(15, 11))\n",
        "\n",
        "  norm = colors.Normalize(vmin=true_quantity.min(), vmax=true_quantity.max())\n",
        "\n",
        "  axs[0, 0].set_title(f\"True {quantity_name}\")\n",
        "  axs[0, 0].plot(z_boundary.real, z_boundary.imag, color=\"red\")\n",
        "  scatter_0 = axs[0, 0].scatter(z_mesh.real, z_mesh.imag, c=true_quantity, marker=\".\", norm=norm)\n",
        "  axs[0, 0].set_aspect(1)\n",
        "\n",
        "  axs[0, 1].set_title(f\"Predicted {quantity_name}\")\n",
        "  axs[0, 1].plot(z_boundary.real, z_boundary.imag, color=\"red\")\n",
        "  scatter_1 = axs[0, 1].scatter(z_mesh.real, z_mesh.imag, c=predicted_quantity, marker=\".\", norm=norm)\n",
        "  axs[0, 1].set_aspect(1)\n",
        "\n",
        "  axs[0, 2].set_title(f\"Delta {quantity_name}\")\n",
        "  axs[0, 2].plot(z_boundary.real, z_boundary.imag, color=\"red\")\n",
        "  scatter_2 = axs[0, 2].scatter(z_mesh.real, z_mesh.imag, c=predicted_quantity - true_quantity, marker=\".\")\n",
        "  axs[0, 2].set_aspect(1)\n",
        "\n",
        "  axs[1, 0].set_axis_off()\n",
        "  axs[1, 1].set_axis_off()\n",
        "  axs[1, 2].set_axis_off()\n",
        "  fig.colorbar(scatter_0, ax=axs[1, 0], orientation='horizontal', fraction=1, shrink=0.9)\n",
        "  fig.colorbar(scatter_1, ax=axs[1, 1], orientation='horizontal', fraction=1, shrink=0.9)\n",
        "  fig.colorbar(scatter_2, ax=axs[1, 2], orientation='horizontal', fraction=1, shrink=0.9)\n",
        "\n",
        "  return fig\n",
        "\n",
        "def plot_impermeability(\n",
        "    z_boundary: NDArray[np.complex64],\n",
        "    v_boundary_pred: NDArray[np.complex64]\n",
        "    ):\n",
        "  plt.ioff()\n",
        "  fig, axs = plt.subplots(1)\n",
        "  axs.plot(z_boundary.real, z_boundary.imag, color=\"red\")\n",
        "  quiver = axs.quiver(z_boundary.real, z_boundary.imag, v_boundary_pred.real, v_boundary_pred.imag)\n",
        "  axs.set_aspect(1)\n",
        "  plt.colorbar(quiver)\n",
        "\n",
        "  return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjtbqrYFgwLh"
      },
      "outputs": [],
      "source": [
        "z_mesh, v_mesh, p_mesh = original_mesh_data(cylinder, 2, 50)\n",
        "z_boundary, n_boundary, v_boundary, p_boundary = original_boundary_data(cylinder, 40)\n",
        "\n",
        "v_mesh_pred, p_mesh_pred = predict(mlp, z_mesh)\n",
        "v_boundary_pred, p_boundary_pred = predict(mlp, z_boundary)\n",
        "\n",
        "validation_metrics(\n",
        "    v_mesh,\n",
        "    v_mesh_pred,\n",
        "    p_mesh,\n",
        "    p_mesh_pred,\n",
        "    n_boundary,\n",
        "    v_boundary_pred,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phrYu74nGcUD"
      },
      "outputs": [],
      "source": [
        "plot_flow_field_comparison(z_boundary, v_mesh, v_mesh_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOm9vCObvqHD"
      },
      "outputs": [],
      "source": [
        "plot_mesh_comparison(z_boundary, z_mesh, v_mesh.real, v_mesh_pred.real, \"$v_x$\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StpqPsP1He7m"
      },
      "outputs": [],
      "source": [
        "plot_mesh_comparison(z_boundary, z_mesh, v_mesh.imag, v_mesh_pred.imag, \"$v_y$\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7HgKKc_HjbR"
      },
      "outputs": [],
      "source": [
        "plot_mesh_comparison(z_boundary, z_mesh, p_mesh, p_mesh_pred, \"p\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLtVGbaLg9ZY"
      },
      "outputs": [],
      "source": [
        "impermeability_figure = plot_impermeability(z_boundary, v_boundary_pred)\n",
        "impermeability_figure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFy3o9L1FeRW"
      },
      "outputs": [],
      "source": [
        "def predicted_stagnation_point_search(shape: PotentialFlowShape, model: torch.nn.Module):\n",
        "\n",
        "  first_theta_min = 20 * np.pi / 180\n",
        "  second_theta_min = -160 * np.pi / 180\n",
        "\n",
        "  def internal_fn(t):\n",
        "    z, _ = shape.get_boundary(t)\n",
        "    v_pred, _ = predict(model, z)\n",
        "    v_mod = np.absolute(v_pred)\n",
        "\n",
        "    return v_mod\n",
        "\n",
        "  first_stagnation = fsolve(internal_fn, first_theta_min)\n",
        "  second_stagnation = fsolve(internal_fn, second_theta_min)\n",
        "  return np.array((first_stagnation, second_stagnation))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GEX8u8NzTOT"
      },
      "outputs": [],
      "source": [
        "predicted_stagnation_point_search(cylinder, mlp) * 180 / np.pi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"experiments\"></a>\n",
        "## Experiments\n",
        "\n",
        "The problem characteristic length is $1$, as we selected a cylinder diameter as such. What we wish to test are different free stream regions. Theoretically, free stream is at $x \\rightarrow \\infty$ and $y \\rightarrow \\infty$, which is impossible to enforce in practice in the physics-informed neural network case. We test three horizons - $3$, $5$ and $7$ - and for each we try with and without SoftAdapt loss aggregation.\n",
        "\n",
        "First, we will compare the results for different horizons without SoftAdapt loss aggregation, then evaluate the effect of it. Let us recall that the validation metrics are mean absolute errors for $M_x$, $M_y$ and continuity equations evaluated on a grid and deviation from perfect impermeability on equidistant points on the boundary. This makes the metrics comparable between different models and horizons.\n",
        "\n",
        "List of experiments:\n",
        "\n",
        "Experiment | Horizon | SoftAdapt | Iterations |\n",
        "| ---      | ---     | ---       | ---        |\n",
        "| 1        | 3       |           | 30k        |\n",
        "| 2        | 3       | x         | 30k        |\n",
        "| 3        | 5       |           | 30k        |\n",
        "| 4        | 5       | x         | 30k        |\n",
        "| 5        | 5       |           | 45k        |\n",
        "| 6        | 7       |           | 30k        |\n",
        "| 7        | 7       | x         | 30k        |\n",
        "| 8        | 7       |           | 45k        |\n",
        "| 9        | 7       | x         | 45k        |\n",
        "\n",
        "We find all horizons with budget 30k, with and without SoftMax aggregation. Horizons $5$ and $7$ were also tested with 45k iterations."
      ],
      "metadata": {
        "id": "SQAM3wVwT3ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"results\"></a>\n",
        "## Results\n",
        "\n",
        "Let us compare the different horizons first. From the loss and validation loss evolutions we notice how increasing the horizon takes us closer and closer to the ideal infinity horizon condition. We do not show it here, but the improvement comes from both partial differential equations and free-stream loss. The impermeability loss is the only loss that is higher only for horizon $3$ and similar for the more distant horizons.\n",
        "\n",
        "| ![losses_horizon.png](https://github.com/a-mazzetto/potential-flow-pinns/blob/main/images/cylinder/losses_horizon.png?raw=1) | ![validation_losses_horizon.png](https://github.com/a-mazzetto/potential-flow-pinns/blob/main/images/cylinder/validation_losses_horizon.png?raw=1) |\n",
        "| :--: | :--: |\n",
        "| Figure 1.1 *Losses for horzion 3 (blue), 5 (red) and 7 (green)* | Figure 1.2 *Validation loss on a pre-defined grid* |\n",
        "\n",
        "Assuming to rule out horzion $3$, below we show the differences between the remaining horizons in terms of velocity and pressure. While horizon $7$ pretty much halves the erros with respect to horizon $5$, we could argue that horizon $5$ is already a good result showing errors of the order of just a few points: $0.00$ to $0.03$ velocity units in the $x$ direction, $-0.01$ to $0.02$ velocity units in the $y$ direction and $-0.01$ to $0.04$ pressure units.\n",
        "\n",
        "| ![final_horizon_5_45kit_vx.png](https://github.com/a-mazzetto/potential-flow-pinns/blob/main/images/cylinder/final_horizon_5_45kit_vx.png?raw=1) | ![final_horizon_7_45kit_vx.png](https://github.com/a-mazzetto/potential-flow-pinns/blob/main/images/cylinder/final_horizon_7_45kit_vx.png?raw=1) |\n",
        "| :--: | :--: |\n",
        "| Figure 2.1 *$v_x$ on a pre-defined grid for horizon $5$* | Figure 2.2 *$v_x$ on a pre-defined grid for horizon $7$* |\n",
        "\n",
        "| ![final_horizon_5_45kit_vy.png](https://github.com/a-mazzetto/potential-flow-pinns/blob/main/images/cylinder/final_horizon_5_45kit_vy.png?raw=1) | ![final_horizon_7_45kit_vy.png](https://github.com/a-mazzetto/potential-flow-pinns/blob/main/images/cylinder/final_horizon_7_45kit_vy.png?raw=1) |\n",
        "| :--: | :--: |\n",
        "| Figure 3.1 *$v_y$ on a pre-defined grid for horizon $5$* | Figure 3.2 *$v_y$ on a pre-defined grid for horizon $7$* |\n",
        "\n",
        "| ![final_horizon_5_45kit_p.png](https://github.com/a-mazzetto/potential-flow-pinns/blob/main/images/cylinder/final_horizon_5_45kit_p.png?raw=1) | ![final_horizon_7_45kit_p.png](https://github.com/a-mazzetto/potential-flow-pinns/blob/main/images/cylinder/final_horizon_7_45kit_p.png?raw=1) |\n",
        "| :--: | :--: |\n",
        "| Figure 4.1 *$p$ on a pre-defined grid for horizon $5$* | Figure 4.2 *$p$ on a pre-defined grid for horizon $7$* |\n",
        "\n",
        "Regarding the SoftAdapt loss aggregation: while it is true it did improve the overall loss, we also noticed that not all the underlying validation losses improved. This is worth further investigation."
      ],
      "metadata": {
        "id": "wol2VmuyVFWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"conclusions\"></a>\n",
        "## Conclusions"
      ],
      "metadata": {
        "id": "-76qYX2AVHU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"references\"></a>\n",
        "## References\n",
        "\n",
        "[1] <a name=\"Ref1\"></a> Fluid Dynamics. Part 1: Classical Fluid Dynamics. Anatoly I. Ruban and Jitesh S. B. Gajjar. ISBN: 9780199681730\n",
        "\n",
        "[2] <a name=\"sobol\"></a> Halton, John H. “On the efficiency of certain quasi-random sequences of points in evaluating multi-dimensional integrals.” Numerische Mathematik 2.1 (1960): 84-90.\n",
        "\n",
        "[3] <a name=\"softadapt\"></a> https://docs.nvidia.com/physicsnemo/latest/physicsnemo-sym/user_guide/theory/advanced_schemes.html SoftAdapt section"
      ],
      "metadata": {
        "id": "JhHHwBAoe4AW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mk89FP30e-Au"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMk7jbQfjtBTCUKzc8RGMtS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}